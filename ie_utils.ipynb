{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import package cells\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from quantulum import parser\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import spacy\n",
    "from __future__ import unicode_literals\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_keyword_found(line,keywords,threshold=80):\n",
    "    \"\"\"Finds whether any of the keywords match is found in the given line within the given threshold\"\"\"\n",
    "    \n",
    "    flag=False\n",
    "    header='nill'\n",
    "    for key in keywords:\n",
    "        if  fuzz.ratio(key.lower(),line.lower().strip())>threshold:\n",
    "            flag=True\n",
    "            header=key\n",
    "            \n",
    "            break\n",
    "\n",
    "    return [flag,header]\n",
    "\n",
    "def find_entity(t):\n",
    "    \n",
    "    \"\"\"Checks the text for country entity\"\"\"\n",
    "    \n",
    "    t=t.decode('utf8')\n",
    "    if t.strip() in all_countries:\n",
    "        #print('from country',t)\n",
    "        return t.strip()\n",
    "    if len(t)>=2:\n",
    "        tokens =  nlp(t)       #print tokens.ents\n",
    "        if len(tokens.ents)!=0:\n",
    "\n",
    "            entity_text=''\n",
    "            for entity in tokens.ents:\n",
    "                #print 'label',entity.label_,'text',entity.text\n",
    "                entity_label=entity.label\n",
    "\n",
    "                if  entity.label_=='GPE' :\n",
    "                    # print 'entities',entity.text\n",
    "                    return entity.text\n",
    "\n",
    "            if entity_text=='':\n",
    "\n",
    "                return 'nill'\n",
    "    else:\n",
    "        return 'nill'\n",
    "            \n",
    "  \n",
    "\n",
    "def remove_non_ascii_2(text):\n",
    "    \n",
    "    \"\"\"Removes non ascii characters from the text (mainly to avoid encoding errors)\"\"\"\"\n",
    "    if text==None:\n",
    "        return\n",
    "    else:\n",
    "       return re.sub(r'[^\\x00-\\x7F]+',' ', text)  \n",
    "\n",
    "def head_body_tagger(file_as_list,headers):\n",
    "    \"\"\"Returns dictionary which returns headers(keys) which are tagged with body(value)\"\"\"\n",
    " \n",
    "    file_as_list=file_as_list+['EOF']  #to capture the last heading.\n",
    "    \n",
    "    header_body={}\n",
    "    header='Title'\n",
    "    header_body['Title']=[]\n",
    "    body=[]\n",
    "    \n",
    "    for line_no in range(0,len(file_as_list)):\n",
    "        flag,current_header=is_keyword_found(file_as_list[line_no],headers,threshold=85)\n",
    "        if flag==False: #Appending to the body list if not the header\n",
    "            \n",
    "            body=body+[file_as_list[line_no]] \n",
    "\n",
    "        else: # Saving the body and resetting the header \n",
    "            print('header found:-',current_header,'---',file_as_list[line_no])\n",
    "\n",
    "            header_body[header]=header_body[header]+[body]\n",
    "             \n",
    "            body=[]\n",
    "            header=current_header\n",
    "            header_body[header]=[]\n",
    "            \n",
    "    return header_body\n",
    "    \n",
    "\n",
    "def get_section(section_name,file_as_list):\n",
    "    \n",
    "    \"\"\"Valid section names \n",
    "    Custody Headers,Fund  Accounting Headers,Fund Administration,Bundled Services,Others\"\"\"\n",
    "    \n",
    "    \"\"\"Returns relevant section content based on the headers\"\"\"\n",
    "    \n",
    "    section_headers=pd.read_csv('/home/sriram/Desktop/AIDE/IE/Projects/ss/custody/eswar/keywords/header_tag_new.csv')\n",
    "    all_headers=list(section_headers['Headers'])\n",
    "    hb_tagged=head_body_tagger(file_as_list,all_headers)\n",
    "    \n",
    "    found_headers=hb_tagged.keys()\n",
    "    \n",
    "    relevant_headers=list(section_headers[section_headers['Section']==section_name]['Headers'])\n",
    "    \n",
    "    found_relevant_headers=[]\n",
    "    for header in found_headers:\n",
    "        if header in relevant_headers :\n",
    "            found_relevant_headers.append(header)\n",
    "    \n",
    "    #print('Relevant Headers',relevant_headers)\n",
    "    #print('Relevant Found Headers:-',found_relevant_headers)\n",
    "    \n",
    "    relevant_body=[]\n",
    "    for header in found_relevant_headers:\n",
    "        \n",
    "        try:\n",
    "            relevant_body=relevant_body+[header]+hb_tagged[header][0]\n",
    "        except:\n",
    "            relevant_body\n",
    "        \n",
    "    return relevant_body,found_relevant_headers,hb_tagged\n",
    "\n",
    " \n",
    "\n",
    "def is_tiered(keyword_indices):\n",
    "    \"\"\"Check if upcoming lines from keywords lines contains tier values\"\"\"\n",
    "    \n",
    "    threshold=2\n",
    "    match_lines=[]\n",
    "    tier_keys=['million','billion','thereafter']\n",
    "    for line_index in keyword_indices:\n",
    "        sub_list=file_as_list[line_index:line_index+15]\n",
    "        for sub_list_line in sub_list:\n",
    "\n",
    "            if any(key in sub_list_line.lower() for key in tier_keys) :\n",
    "                #print line_index,sub_list_line\n",
    "                match_lines.append(sub_list_line)\n",
    "            \n",
    "\n",
    "    if len(set(match_lines))>=threshold: #for more than one incidence of co-occurence event.\n",
    "\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def tier_extract(keyword_indices,file_as_list):\n",
    "    \"\"\"Extract tier lines from the text\"\"\"\n",
    "    \n",
    "    tier_keys=['million','billion','thereafter','Asset']\n",
    "    basis_lis=[]\n",
    "    for line_index in keyword_indices:\n",
    "        sub_list=file_as_list[line_index:line_index+7]\n",
    "        present_iter=0\n",
    "        normal_iter=0\n",
    "        last_iter_temp=[]\n",
    "        for sub_list_line in sub_list:\n",
    "            normal_iter=normal_iter+1\n",
    "            #if any(key in sub_list_line.lower() for key in tier_keys) and len(re.findall(r'[^$]\\s+\\d{0,2}\\.\\d{1,4}',sub_list_line))>0:\n",
    "            if any(key in sub_list_line.lower() for key in tier_keys) and len(re.findall(r'[^$]\\s+\\d\\.\\s*\\d*',sub_list_line))>0:\n",
    "     \n",
    "                last_iter_temp=normal_iter\n",
    "                print sub_list_line\n",
    "                fraction=re.findall(r'\\d*\\/\\d*',sub_list_line)\n",
    "                if len(fraction)>0:\n",
    "                    print('fraction_founded ........',sub_list_line)\n",
    "                basis_lis.append(sub_list_line)\n",
    "        if len(basis_lis)>0:\n",
    "            a=0\n",
    "            return remove_dup(basis_lis)\n",
    "                \n",
    "                \n",
    "            \n",
    "    return remove_dup(basis_lis)\n",
    "\n",
    "\n",
    "\n",
    "def tier_extract_original(bps_list):\n",
    "   \"\"\" Extract tier values [min,max,value] from the worded strings\"\"\"\n",
    "    \n",
    "   base_tier_keys=['first','$0']\n",
    "   middle_tier_keys=['next','second','between']\n",
    "   middle_tier_key_range=['to','between','and','-']\n",
    "   top_tier_keys=['thereafter','excess','over']\n",
    "   mult=1\n",
    "   tier_structure=[]\n",
    "    \n",
    "   tier_limit=0;bps_value=0;tier_limit_temp=0\n",
    "\n",
    "   for line in bps_list: \n",
    "        \n",
    "      # print('line',line)\n",
    "       try: \n",
    "           if '-' in line:\n",
    "               val=re.findall(r'\\$\\s*(\\d*\\.*\\,*\\d*)',line)[1]\n",
    "               tier_limit=float(val.replace(',',''))\n",
    "\n",
    "           else:\n",
    "               print line\n",
    "               val=re.findall(r'\\$\\s*(\\d*\\.*\\,*\\d*)',line)[0]\n",
    "               tier_limit=float(val.replace(',',''))\n",
    "       except:\n",
    "            tier_limit=tier_limit_temp\n",
    "        \n",
    "       #print('tier limit',tier_limit)\n",
    "       #incase of 1/100 .3 basis point\n",
    "       fraction=re.findall(r'\\d\\/\\d*',line)\n",
    "       if len(fraction)==2:\n",
    "           print('fraction_founded ........')\n",
    "           try:\n",
    "               frac_mul=float(fraction[0].split('/')[0])/float(fraction[0].split('/')[1])\n",
    "           except:\n",
    "               print('fraction exception')\n",
    "               frac_mul=1\n",
    "       else:\n",
    "           frac_mul=1\n",
    "            \n",
    "       #incase of percentage % in line\n",
    "       if '%' in line:            \n",
    "           bps_value=float(re.findall(r'[^$]\\s+(\\d{0,1}\\.\\s*\\d*)',line)[0].replace(' ',''))*(1/100.0)*frac_mul #percentage and fraction mul.\n",
    "       else:\n",
    "           bps_value=float(re.findall(r'[^$]\\s+(\\d{0,1}\\.\\s*\\d*)',line)[0].replace(' ',''))*frac_mul #only fraction mul.\n",
    "        \n",
    "    \n",
    "       if 'billion' in line.lower():\n",
    "           mult=1000\n",
    "       else:\n",
    "           mult=1\n",
    "            \n",
    "       if any(key in line.lower() for key in base_tier_keys):\n",
    "           min_value=0\n",
    "           max_value=tier_limit*mult\n",
    "           print('max',max_value)\n",
    "           tier_limit_temp=tier_limit_temp+max_value\n",
    "           print('base')\n",
    "            \n",
    "       elif any(key in line.lower() for key in middle_tier_keys)  or ('over' in line.lower() and '-' in line.lower()):\n",
    "           \n",
    "           print('middle',line)\n",
    "           min_value=tier_limit_temp\n",
    "           max_value=tier_limit*mult+tier_limit_temp\n",
    "           #tier_limit_temp+=tier_limit\n",
    "           tier_limit_temp=max_value\n",
    "            \n",
    "           if any(key in line.lower() for key in middle_tier_key_range):\n",
    "               min_value=float(re.findall(r'\\$\\s*(\\d*)',line)[0])*mult\n",
    "               max_value=float(re.findall(r'\\$\\s*(\\d*)',line)[1])*mult\n",
    "               tier_limit_temp+=max_value\n",
    "        \n",
    "                \n",
    "       else:\n",
    "           if 'over' not in line.lower() and 'excess' not in line.lower():\n",
    "               min_value=tier_limit_temp*mult\n",
    "           else:\n",
    "                min_value=tier_limit*mult\n",
    "                \n",
    "           max_value='Above'\n",
    "           print('top tier')\n",
    "       try:\n",
    "           print(bps_list)\n",
    "           print(min_value,max_value,bps_value)\n",
    "           tier_structure.append([min_value,max_value,bps_value])\n",
    "       except:\n",
    "           continue;\n",
    "            \n",
    "   return tier_structure\n",
    "\n",
    "def remove_dup(bps_list):\n",
    "    \n",
    "    \"\"\"Remove duplicate entries from the list\"\"\"\n",
    "    fin_list = [] \n",
    "    for txt in bps_list: \n",
    "        if txt not in fin_list: \n",
    "            fin_list.append(txt) \n",
    "            \n",
    "    return fin_list\n",
    "\n",
    "def keyword_found_indices(file_as_list,keywords):\n",
    "    \"\"\"\"Returns the indices of the file where the keywords are as substring of a line\"\"\"\n",
    "    \n",
    "    found_indices=[]\n",
    "\n",
    "    for i in range(len(file_as_list)):\n",
    "        \n",
    "        if any(key in file_as_list[i] for key in keywords):\n",
    "            \n",
    "            found_indices.append(i)\n",
    "    return found_indices\n",
    "\n",
    "def keyword_found_indices2(file_as_list,keywords):\n",
    "    \"\"\"\"Returns the indices of the file where the keywords are as substring of a line\"\"\"\n",
    "    \n",
    "    found_indices=[]\n",
    "\n",
    "    for i in range(len(file_as_list)):\n",
    "        \n",
    "        if is_keyword_found(file_as_list[i].strip(),keywords,threshold=75)[0]==True:\n",
    "            \n",
    "            found_indices.append(i)\n",
    "    return found_indices\n",
    "\n",
    "def is_keyword_found(line,keywords,threshold=80):\n",
    "    \"\"\"Finds whether any of the keywords match is found in the given line within the given threshold\"\"\"\n",
    "    \n",
    "    flag=False\n",
    "    header='nill'\n",
    "    for key in keywords:\n",
    "        if  fuzz.ratio(key.lower().strip(),line.lower().strip())>threshold:\n",
    "            flag=True\n",
    "            header=key\n",
    "            return [flag,header]\n",
    "    return False,'nill'\n",
    "            \n",
    "         \n",
    "\n",
    "def find_nearby_quantity(found_indices,file_as_list,dtype,false_keys=[],window=5):\n",
    "    \"\"\"Finds the nearby numerical quantity of dtype(currency/percentage) in the given window of \n",
    "        the indices where keywords are found\"\"\"\n",
    "    \n",
    "    basis_lis='NA'\n",
    "    cdollar_keywords=['C$','CAD','Can$']\n",
    "    false_keys=[]\n",
    "    fl=0\n",
    "    currency_type=''\n",
    "    for line_index in found_indices:\n",
    "            \n",
    "          \n",
    "            sub_list=file_as_list[line_index:line_index+window]\n",
    "            #print(sub_list[0])\n",
    "            \n",
    "            if 'no charge' in sub_list[0].lower():\n",
    "                return 'No Charge'\n",
    "            if 'waived' in sub_list[0].lower():\n",
    "                return 'Waived'\n",
    "            \n",
    "            \n",
    "            for i in range(len(sub_list)):\n",
    "\n",
    "                #if any(key in sub_list[i].lower() for key in bps_keys):\n",
    "                  \n",
    "                for k in range(i,len(sub_list)):\n",
    "                    bullets=re.findall(r\"^\\d{1}[.][^\\d]\",sub_list[k])\n",
    "                    quants=[]\n",
    "                    try:\n",
    "                        quants=parser.parse(sub_list[k])\n",
    "                       # print('quants',quants)\n",
    "                    except:\n",
    "                        #expression like these 44.0/44 throw quantum error..so capturing those lines \n",
    "                        print('Quants exception',sub_list[k]) \n",
    "                        #basis_lis.append(sub_list[k]) #first instance\n",
    "                        return 'NA'\n",
    "\n",
    "                    for q in range(len(quants)):  \n",
    "\n",
    "                        if dtype=='percentage':\n",
    "                            if len(bullets)==0 and len(parser.parse(sub_list[k]))>0 and ('dollar' not in parser.parse(sub_list[k])[q].unit.name)  and any(key not in sub_list[k].lower() for key in false_keys):\n",
    "                                #print(parser.parse(sub_list[k])[0])\n",
    "                                basis_lis.append(parser.parse(sub_list[k])[0]) #first instance\n",
    "\n",
    "                                fl=1\n",
    "                                break\n",
    "\n",
    "                        elif dtype=='currency':\n",
    "                            \n",
    "                            \n",
    "                            if len(bullets)==0 and len(parser.parse(sub_list[k]))>0:\n",
    "                                print('currency found')\n",
    "                                if any(key in file_as_list[i] for key in cdollar_keywords):\n",
    "                                    currency_type='Canadian Dollar'\n",
    "                                else:\n",
    "                                    currency_type='US Dollar'\n",
    "                                    \n",
    "                                basis_lis=parser.parse(sub_list[k])[0].value #first instance\n",
    "                                \n",
    "                                return basis_lis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml  parsing for tables from OCR output\n",
    "country_list=[]\n",
    "transaction_fee=[]\n",
    "basis_point=[]\n",
    "dois=[]\n",
    "missed=[]\n",
    "files=os.listdir('.//xmls2')\n",
    "n=0\n",
    "for doi in files:\n",
    "\n",
    "    tree = ET.parse(\"/home/sriram/Desktop/AIDE/IE/Projects/ss/custody/eswar/xml/xmls2/\"+doi)\n",
    "    #tree=ET.parse(\"/home/sriram/Desktop/AIDE/IE/Projects/ss/custody/eswar/xml/ss_out_without_unlined/out/Bi2i_0000.xml\")\n",
    "    root = tree.getroot()\n",
    "    no_pages=len(root)\n",
    "    print 'n',n,doi\n",
    "    n=n+1\n",
    "\n",
    "    for page in range(no_pages):\n",
    "        #print page\n",
    "        for i in range(len(root[page][0])):\n",
    "\n",
    "            if root[page][0][i].tag=='line':\n",
    "               # print root[page][0][i][0]\n",
    "                basis_pt=[]\n",
    "                transaction_fe=[]\n",
    "\n",
    "\n",
    "\n",
    "            if root[page][0][i][0].text!=None:\n",
    "                text=remove_non_ascii_2(root[page][0][i][0].text)\n",
    "                text_found=find_entity(text)\n",
    "                if text!=None and text_found!='nill' and text_found!=None:\n",
    "                    country_list.append(text_found)\n",
    "                    dois.append(doi)\n",
    "                    #print text_found\n",
    "                    text_next=remove_non_ascii_2(root[page][0][i+1][0].text)\n",
    "\n",
    "                    if text_next!=None and '.' in text_next:  #if basis point is found next\n",
    "                        basis_pt=text_next\n",
    "\n",
    "                        text_next_next=root[page][0][i+2][0].text\n",
    "                        if text_next_next!=None and '$' in text_next_next:\n",
    "                            transaction_fe=remove_non_ascii_2(text_next_next)\n",
    "\n",
    "                    elif text_next!=None and '$' in text_next:  #if transaction charge is found next\n",
    "                        transaction_fe=text_next\n",
    "\n",
    "                        text_next_next=root[page][0][i+2][0].text\n",
    "                        if text_next_next!=None and '.' in text_next_next:\n",
    "                            basis_pt=remove_non_ascii_2(text_next_next)\n",
    "\n",
    "\n",
    "\n",
    "                    if basis_pt!=[]:\n",
    "\n",
    "                        basis_point.append(basis_pt)\n",
    "                    else:\n",
    "                        basis_point.append('Nill')\n",
    "                    if transaction_fe!=[]:\n",
    "                        transaction_fee.append(transaction_fe)\n",
    "                    else:\n",
    "                        transaction_fee.append('Nill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
