{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df=pd.read_csv('Tweets_hashtags_2020_jan_feb.csv',encoding='unicode_escape')\n",
    "print('data loaded')\n",
    "df=df[df['Screen Name']!='MichaeLawbzpc\\n']\n",
    "tweets=list(df['Tweets'])\n",
    "\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "  tweet_embeddings = session.run(embed(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet vectors using Universal Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "for i in np.arange(1,1.2,0.1):\n",
    "    \n",
    "    clustering = DBSCAN(eps=i, min_samples=100,n_jobs=-1).fit(tweet_embeddings)\n",
    "    print('no. of clusters',len(pd.Series(clustering.labels_).unique()),' eps:-',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0.9\n",
    "clustering = DBSCAN(eps=i, min_samples=100,n_jobs=-1).fit(tweet_embeddings)\n",
    "print('no. of clusters',len(pd.Series(clustering.labels_).unique()),' eps:-',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clusters=len(pd.Series(clustering.labels_).value_counts())\n",
    "\n",
    "df['clusters_embeddings']=clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word clouds for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "for c in range(-1,no_clusters-1):\n",
    "    print('Cluster No.',c)\n",
    "    hts=list(df[df['clusters_embeddings']==c]['Hashtags'])\n",
    "\n",
    "    hashes=[]\n",
    "    for ht in  hts:\n",
    "        for h in eval(ht):\n",
    "            hashes.append(h.strip())\n",
    "\n",
    "    string_hash=' '.join(hashes)\n",
    "\n",
    "    hash_values=pd.Series(hashes).value_counts()\n",
    "\n",
    "    hval=hash_values.reset_index()\n",
    "\n",
    "    #wordcloud plot\n",
    "    d = {}\n",
    "    for a, x in hval.values:\n",
    "        d[a] = x\n",
    "\n",
    "    wordcloud = WordCloud(max_font_size=40)\n",
    "    wordcloud.generate_from_frequencies(frequencies=d)\n",
    "    plt.figure(figsize=(100,100))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.savefig('Cluster_jan_feb'+str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['clusters_embeddings']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(df['clusters_embeddings']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster Analysis\n",
    "#Cluster -1 :-Outliers by algo\n",
    "#Cluster 0:- General litigation\n",
    "#Cluster 1:- Week day tweets\n",
    "#Cluster 2:- whistleblowers\n",
    "#Cluster 3:- boxing\n",
    "#Cluster 4:- health related(teen Vaping,e-cigarette,Big Tobacco)\n",
    "#Cluster 5:- Spanish,french..tweets(non-english)\n",
    "#Cluster 6:- Injury, Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
